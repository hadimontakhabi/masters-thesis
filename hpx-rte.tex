\chapter{HPX-RTE}
\label{sec:HPX-RTE}
ParalleX and HPX are both parts of the eXascale Programming Environment and System Software (XPRESS)~\cite{huck2013early,brightwell2013xpress} project funded by the Department of Energy (DOE).

The goals of XPRESS project are~\cite{xpress}:
\begin{itemize}
\item Enable exascale performance capability for Department of Energy applications
\item Develop a software stack, ``OpenX'', for future Department of Energy computing systems
\item Provide programming models, languages, environments, and tools for expressing system and application software for exascale
\end{itemize}

This project is a collaboration between Sandia National Laboratories (SNL), Indiana University (IU), Lawrence Berkeley National Laboratory (LBNL), Louisiana State University (LSU), Oak Ridge National Laboratory (ORNL), University of Houston (UH), University of North Carolina at Chapel Hill/RENCI (UNC/RENCI), and University of Oregon (UO).

Figure \ref{fig:openx} illustrates the OpenX software architecture. Support for legacy applications, and specifically support for MPI applications is this thesis' target.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.36]{images/openx.png}
\caption[The OpenX Software Architecture]{The OpenX Software Architecture}
\label{fig:openx}
\end{figure}


\section{Design Principles}
\label{sec:design}
The main goal of this project that led to the development of HPX-RTE was to provide facilities for MPI applications to compile and run the exact same way in exascale software stack environment and be compatible with HPX runtime environment. This goal is achieved by replacing the current runtime environment (ORTE) with a new runtime environment developed from scratch to take advantage of the API provided by HPX. This choice was made by having a set of design principles in mind:

\begin{itemize}
\item \textbf{Modularity}\\
  Utilizing the modular structure of the Open MPI project, there is a framework dedicated to the runtime environment (rte) in OMPI layer. This framework is designed to provide the interfce necessary for different runtime environments (Figure \ref{fig:MCA-hpx-rte}). This allows different runtime environments to coexist independently inside the Open MPI project and be chosen by users based on their environment and application needs and priorities.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{images/MCA-hpx-rte.png}
\caption[Modular Component Architecure with rte Framework]{Modular Component Architecure with rte Framework}
\label{fig:MCA-hpx-rte}
\end{figure}
  
\item \textbf{Functionality}\\
  The main focus of this work is to have the MPI applications work on HPX infrastructure. To do this, most of the effort is put on having the functionality in place before tackling any possible consequences such as its effect on performance, efficiency, or power consumption. Although every possible consideration has been taken into account not to introduce sources of performance degradation, such potential side effects could still happen. Those may be the topic for other studies after having the new runtime environment in place and functional.
  
\item \textbf{Simplicity}\\
  Simplicity is a key factor is the design of HPX-RTE. We have avoided introducing any unnecessary algorithm or functionality. Implementing the required API, we have tried to keep implementation as simple and straightforward as possible.

\item \textbf{Code Reuse}\\
  HPX-RTE relies on a number of advanced features and concepts provided by the HPX API. Therefore, there was no need to reimplement what was already done. We have tried to take those features to a new level by integrating them tightly into the implementation of HPX-RTE.

\end{itemize}


\section{Architecture}
\label{sec:architecture}
HPX-RTE is designed to replace the functionality of current Open MPI runtime (ORTE). Figure \ref{fig:open-mpi-layers-hpx-rte} illustates the logical layers of Open MPI software using \mbox{HPX-RTE} as its runtime environment. The modular component architecture of Open MPI facilitates the implementation of this design.

The required functinality for HPX-RTE is implemented as a component of the runtime environment (rte) framework (Figure \ref{fig:MCA-hpx-rte}) in OMPI layer. This design dictates a number of changes outside the rte framework. We will discuss those changes in the implementation section.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.45]{images/open-mpi-layers-hpx-rte.png}
\caption[Open MPI Layers using HPX-RTE]{Open MPI Layers using HPX-RTE}
\label{fig:open-mpi-layers-hpx-rte}
\end{figure}


\section{Runtime Environement Requirements}
\label{sec:rte-requirements}
The rte framework inside OMPI layer of Open MPI defines a required set of data structures and functions that every runtime environment component needs to provide implementation for~\cite{gabriel04:_open_mpi}:

\subsection{Process Name Objects and Operations}

\begin{enumerate}
\item \verb|ompi_jobid_t| \textbf{and} \verb|ompi_vpid_t|\\
  These two need to be defined as integer types. The jobid must be unique for a given \verb|MPI_COMM_WORLD| capable of connecting to another \verb|OMPI_COMM_WORLD| and the vpid will be the process's rank in \verb|MPI_COMM_WORLD|.

\item \verb|ompi_process_name_t|\\
  This is a struct that must contain at least two fields of type integer:
  \begin{enumerate}
  \item \verb|ompi_jobid_t| jobid
  \item \verb|ompi_vpid_t| vpid
  \end{enumerate}

\item \verb|OMPI_NAME_PRINT|\\
  When given a pointer to \verb|ompi_process_name_t|, this macro has to print a process name. The output format has to be a single string representing the name. This function should be thread-safe for multiple threads to call simultaneously.
  
\item \verb|OMPI_PROC_MY_NAME|\\
  A pointer to a global variable containing the \verb|ompi_process_name_t| for this process.
  
\item \verb|OMPI_NAME_WILDCARD|\\
  A wildcard name.
  
\item \verb|ompi_rte_compare_name_fields|\\
  A function used to compare fields in the \verb|ompi_process_name_t| struct. The function prototype must be of the form:
  \begin{lstlisting}[language=C]
  int ompi_rte_compare_name_fields(
        ompi_rte_cmp_bitmask_t mask,
        ompi_process_name_t *name1,
        ompi_process_name_t *name2);
  \end{lstlisting}
  The bitmask must be defined to indicate the fields to be used in the comparison. Fields not included in the mask must be ignored.
        Supported bitmask values must include:
        \begin{enumerate}
          \item \verb|OMPI_RTE_CMP_JOBID|
          \item \verb|OMPI_RTE_CMP_VPID|
          \item \verb|OMPI_RTE_CMP_ALL|
        \end{enumerate}

      \item \verb|uint64_t ompi_rte_hash_name|\textbf{(name)}\\
        Return a string hash uniquely representing the \verb|ompi_process_name| passed in.

      \item \verb|OMPI_NAME|\\
        An OPAL Data Packing Subsystem(DSS) constant for a handler already registered to serialize/deserialize an \verb|ompi_process_name_t| structure.

\end{enumerate}


\subsection{Collective Objects and Operations}

\begin{enumerate}
\item \verb|ompi_rte_collective_t|\\
  An OPAL object used during the runtime environment collective operations such as modex and barrier. This must be of type \verb|opal_list_item_t| and contain the following fields:
  
  \begin{enumerate}
  \item \verb|int32_t| id
  \item \verb|bool| active: A flag that user can poll on to know when collective operation has completed. If a user callback function is provided, this needs to be set to false just prior to calling it.
  \end{enumerate}

\item \verb|ompi_rte_modex|\\
  A function that performs an exchange of endpoint information to wire up the MPI transports. The function prototype must be of the form:
  \begin{lstlisting}[language=C]
  int ompi_rte_modex(ompi_rte_collective_t *coll);
  \end{lstlisting}
  At the completion of the modex operation, the \verb|coll->active| flag must be set to false, and the endpoint information must be stored in the modex database.
  This function must have barrier semantics across the \verb|MPI_COMM_WORLD| of the calling process.

\item \verb|ompi_rte_barrier|\\
  A function that performs a barrier operation within the RTE. The function prototype must be of the form:
  \begin{lstlisting}[language=C]
  int ompi_rte_barrier(ompi_rte_collective_t *coll);
  \end{lstlisting}
  At the completion of the barrier operation, the \verb|coll->active| flag must be set to false.
\end{enumerate}


\subsection{Process Information Structure}

\begin{enumerate}
\item \verb|ompi_process_info_t|
  A structure containing information about the current process. The following fields are mandatory within the struct:
  \begin{enumerate}
  \item \verb|app_num|
  \item \verb|pid|\\
    The current process's id. This should be the same as the output of getpid() function.
  \item \verb|num_procs|\\
    Number of processes in this job (ie, \verb|MPI_COMM_WORLD|).
  \item \verb|my_node_rank|\\
    Relative rank on local node to other peers this runtime instance knows about. In a static job this will be \verb|my_local_rank|.
  \item \verb|my_local_rank|\\
    Relative rank of the process on the local node with other peers in this job (ie, \verb|MPI_COMM_WORLD|).
  \item \verb|num_local_peers|\\
    Number of local peers (peers in \verb|MPI_COMM_WORLD| on the same node).
  \item \verb|my_hnp_uri|
  \item \verb|peer_modex| \\
    This is a collective id for the modex operation.
  \item \verb|peer_init_barrier| \\
    A collective id for the barrier during \verb|MPI_Init|.
  \item \verb|peer_fini_barrier| \\
    A collective id for the barrier during \verb|MPI_Finalize|.
  \item \verb|job_session_dir|
  \item \verb|proc_session_dir|
  \item \verb|nodename| \\
    A string representation for the name of the node this process is located on.
  \item \verb|cpuset|
  \item \verb|ompi_process_info| \\
    A global instance of the \verb|ompi_process_t| structure.
  \end{enumerate}
  
\item \verb|ompi_rte_proc_is_bound|\\
  A global boolean that will be set to true if the runtime bound the process to a particular core or set of cores. Otherwise, it will be false.
\end{enumerate}


\subsection{Error-Handling Objects and Operations}
\begin{enumerate}
\item
  \begin{lstlisting}[language=C]
  void ompi_rte_abort(int err_code, char *fmt, ...)
  \end{lstlisting}
  Abort the current process with the specified error code and message.

\item
  \begin{lstlisting}[language=C]
  int ompi_rte_abort_peers(ompi_process_name_t *procs,
                           size_t nprocs)
  \end{lstlisting}
  Abort the specified list of peers.
  
\item \verb|OMPI_ERROR_LOG(rc)|\\
  This is a macro that prints the error message regarding the given return code.
  
\item \verb|ompi_rte_register_errhandler|\\
  A function to register a callback function for the runtime environment to report asynchronous errors to the caller.
\end{enumerate}


\subsection{Initializing and Finalizing Objects and Operations}

\begin{enumerate}
\item 
  \begin{lstlisting}[language=C]
  int ompi_rte_init(int *argc, char ***argv);
  \end{lstlisting}
  This function initializes the runtime environment.
  
\item 
  \begin{lstlisting}[language=C]
  int ompi_rte_finalize(void);
  \end{lstlisting}
  This function finalizes the runtime environment.
  
\item
  \begin{lstlisting}[language=C]
  void ompi_rte_wait_for_debugger(void);
  \end{lstlisting}
  This function is called during \verb|MPI_Init|. It is used to wait for debuggers to do their pre-MPI attachment. This function will not block if no debugger is attached.
\end{enumerate}


\subsection{Database Operations}
\begin{enumerate}
\item
  \begin{lstlisting}[language=C]
  int ompi_rte_db_store(const ompi_process_name_t *proc,
                        const char *key,
                        const void *data,
                        opal_data_type_t type);
  \end{lstlisting}
  This function is used to store modex and other data in a local database. It is primarily used for storing modex data. The implementation of this function must store a copy of the data provided. The data is not guaranteed to be valid after return from the call.

\item
  \begin{lstlisting}[language=C]
  int ompi_rte_db_fetch(const struct ompi_proc_t *proc,
                        const char *key,
                        void **data,
                        opal_data_type_t type);
  \end{lstlisting}
  This function is used to fetch modex and other data from the database. Fetch accepts an \verb|ompi_proc_t|.

\item
  \begin{lstlisting}[language=C]
  int ompi_rte_db_fetch_pointer(
                  const struct ompi_proc_t *proc,
                  const char *key,
                  void **data,
                  opal_data_type_t type);  
  \end{lstlisting}
    
\item Pre-defined database keys (with associated values after \verb|rte_init|)
  \begin{enumerate}
    \item \verb|OMPI_DB_HOSTNAME|
    \item \verb|OMPI_DB_LOCALITY|
  \end{enumerate}
\end{enumerate}

%\subsection{Communication Support}

\section{Implementation}
\label{sec:implementation}

We have explained in previous sections that Open MPI has an abstraction layer for runtime environments. HPX-RTE provides an implementation of that abstraction layer using HPX features and facilities.

\subsection{HPX-RTE Requirements}
Since both Open MPI and HPX project are under active development, we decided to fix our implementation target to specific versions of these two software projects during the development cycle. HPX-RTE was built on a fork of Open MPI version 1.8 branch. We used the released HPX version 0.9.10. HPX library is dependent on the Boost libraries. Boost libraries version 1.55.0 were installed on the development machines.
To summerize, users will need to have Open MPI 1.8, HPX 0.9.10, and Boost 1.55.0 as requirements to take advantage of HPX-RTE.

\subsection{Features}
Some of the main implementation features of HPX-RTE include:

\begin{enumerate}
\item \textbf{Distributed Database}\\
  For the runtime environment, we need to store a set of key-value pairs in a database accessible to all the localities participating in a job. Utilizing HPX actions, HPX-RTE stores the data on local node at the time a store operation is issued. We have used singly linked lists to store the data. However, to retrieve the data the processes take advantage of HPX actions (remote function calls) to fetch the stored data from the database on the locality it is stored on. In theory, this database scheme eliminates the bottleneck of accessing a centralized database, or storing copies of all the database entries in every single locality. Therefore, it distributes the database, uses less space, and makes accessing the stored data faster. Listing \ref{lst:put} and \ref{lst:get} demononstrate two funtions (\verb|hpx_rte_cpp_put| and \verb|hpx_rte_cpp_get|) that store and fetch operations are built based on.

  \begin{lstlisting}[language=C, frame=single, basicstyle=\footnotesize, caption=Put Functionality\label{lst:put}]
  int rte_hpx_cpp_put(char* key, int keysize,
                      char* val, int valsize)
  {
    std::vector<char> vectorval (val, val + valsize);
    rte_hpx_put( std::string (key), vectorval );
    return 0;
  }
  \end{lstlisting}

\item \textbf{Barrier Semantics}\\
  HPX strives not to have barriers that would stall all processes or threads for a particular operation to be completed. Instead, there are fine-grained mechanisms available for synchronization. However, a barrier (global synchronization point) is a requirement for the runtime component in Open MPI. We have developed an algorithm that implements barrier semantics by making effective use of HPX actions and remote asynchronous function calls (Listing \ref{lst:barrier}). We create a vector of futures with its size equal to the number of localities. Then, we asynchronously call an action to add one to a globally shared atomic counter on each locality. When this counter reaches the total number of localities, the synchronization is accomplished.

  
  \begin{lstlisting}[language=C, frame=single, basicstyle=\footnotesize, caption=Get Functionality using HPX Asynchronous Actions\label{lst:get}]
  int rte_hpx_cpp_get( int vpid, char* key, char** value )
  {
    hpx::naming::id_type const& node =
      rte_hpx_cpp_get_locality_from_vpid (vpid);
    std::string stringkey = key;
    int counter;
    *value = NULL;
    std::vector<char> temp_vector =
      hpx::async<rte_hpx_get_action>(node, stringkey).get();
    if (temp_vector.size() != 0) {
      char *temp_value = (char *) malloc (temp_vector.size());
      memcpy(temp_value, temp_vector.data(), temp_vector.size());
      *value = temp_value;
    }
    return (int) temp_vector.size();
  }
  \end{lstlisting}


\item \textbf{Mapping Localities to Ranks}\\
  HPX uses the notion of locality. ``A locality represents a set of hardware resources with bounded, finite latencies.''~\cite{kaiser2014hpx} MPI uses ranks. We have provided a translation table from HPX localities to mpi ranks.

\item \textbf{Populating Internal Data Structures}\\
  A number of internal data structures need to be initialized within the runtime environment before starting the MPI initialization. This is done in the runtime initialization function.
  
  \begin{lstlisting}[language=C, frame=single, basicstyle=\footnotesize, caption=Barrier Implementation\label{lst:barrier}]
  void rte_hpx_barrier()
  {
    std::vector<hpx::naming::id_type> localities =
    hpx::find_all_localities();
    std::vector<hpx::lcos::future<void> > futures;
    futures.reserve(localities.size());
    BOOST_FOREACH(hpx::naming::id_type const& node, localities) {
      futures.push_back(hpx::async<rte_hpx_one_action>(node));
    }
    hpx::wait_all(futures);
    while(sum != (int) localities.size()) {
    }
    sum = 0;
    return;
  }
  \end{lstlisting}
  
\item \textbf{Populate Key-Value Pairs in the Database}\\
  Since the defined abstraction layer is not implemented perfectly, there was a number of key-value pairs that were needed to be populated into the database. In theory, this shouldn't be the case. But this is a matter of implementation practices. For instance, some of these key-value pairs are set in ORTE layer (which is supposedly an independent layer) and used in OMPI layer. Identifying and extracting those pairs was a challenging task.

\item \textbf{Intercepting C Standard Output}\\
  In order to see the output from C part of the code, we needed to intercept the printf function in C and send its output to C++ output stream. Listing \ref{lst:printf} shows our implementation of print function. This technique can be used for any other standard output function in C.

  \begin{lstlisting}[language=C, frame=single, basicstyle=\footnotesize, caption=Intercepting printf\label{lst:printf}]
  int printf(const char *restrict format, ...)
  {
    char     out[1024];
    va_list  args;
    va_start ( args, format );
    vsnprintf ( out, 1024, format, args );
    va_end   ( args );
    rte_hpx_cpp_printf(out);
    return (strlen(out));
  }
  \end{lstlisting}

\item \textbf{Automatic Boost and HPX Detection}\\
  Since Boost and HPX are required to be installed before the installation of Open MPI with HPX-RTE, we modified the configure logic of Open MPI to detect the installation paths of these libraries and accordingly modify the generated make files with appropriate compilation and linkage flags.
\item \textbf{C and C++ Compatibility}\\
  The majority of the Open MPI source code is written in C language. HPX is completely written in C++ with extensive use of recent C++14 features. This definitely causes compatibility issues which needed to be handled in the code. For instance, Listing \ref{lst:headers} illustrates an example on how different headers are made visible to either C or C++ compiler. 
  \begin{lstlisting}[language=C, frame=single, basicstyle=\footnotesize, caption=Exposing Different Headers to C and C++ Compilers\label{lst:headers}]
  #include <stdio.h>
  #include <stdlib.h>
  #include <unistd.h>

  #if defined(__cplusplus)
  #include <hpx/hpx_main.hpp>
  #include <hpx/hpx_finalize.hpp>
  #include <hpx/include/iostreams.hpp>
  #endif
  \end{lstlisting}

  The same technique is applied for functions that are written in C or C++. We also utilized \textbf{extern ``C''} to make function names in C++ have C linkage (compiler does not mangle the name) so that client C code can link to the functions using a C compatible header file that contains just the declaration of those functions. Listing \ref{lst:functions} illustrates the technique. Note that this listing does not contain all the function prototypes in the source code.
  
  We also had to use C++ to compile a number of components that were initially compiled by the C compiler in Open MPI.

  \begin{lstlisting}[language=C, frame=single, basicstyle=\footnotesize, caption=Exposing Different Functions to C and C++ Compilers\label{lst:functions}]
    #if defined(__cplusplus)
    extern ``C'' {
      struct Node{
        struct Node *next;
        std::string key;
        std::vector<char> value;
        int originator;
      };
      typedef struct Node Node;
      int rte_hpx_vpid(void);
      int rte_hpx_num_localities(void);
      void rte_hpx_local_map(void);
      void rte_hpx_barrier(void);
    }
    #endif
  \end{lstlisting}

\item \textbf{Communication Protocols}\\
  Current implementation of HPX-RTE supports Transmission Control Protocol(TCP)~\cite{1092259} and Infiniband~\cite{infiniband2000infiniband} communication protocol.
\end{enumerate}

\iffalse
current status:
- restrctions: - one rank per locality
- launch with ssh

\subsection{Challenges}
However, the runtime system abstraction layer in Open MPI is imperfect in practice. There is a lack of comprehensive documentation for it, and some of the interface independences were not implemented as they were designed.
C and C++
Imperfect Abstraction in ORTE
Fundamental Differences

\fi
