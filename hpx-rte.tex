\chapter{HPX-RTE}
\label{sec:HPX-RTE}

ParalleX and HPX are both parts of the eXascale Programming Environment and System Software (XPRESS)~\cite{huck2013early,brightwell2013xpress} project funded by the Department of Energy.

The goals of XPRESS project are:
\begin{itemize}
\item ``Enable exascale performance capability for current and future Department of Energy applications
\item Develop and deliver a practical computing system software X‐stack, ``OpenX'', for future practical DOE computing systems
\item Provide programming methods, environments, languages, and tools for effective means of expressing application and system software for portable exascale system execution''~\cite{xpress}
\end{itemize}

This project is a collaboration between Sandia National Laboratories (SNL), Indiana University (IU), Lawrence Berkeley National Laboratory (LBNL), Louisiana State University (LSU), Oak Ridge National Laboratory (ORNL), University of Houston (UH), University of North Carolina at Chapel Hill/RENCI (UNC/RENCI), and University of Oregon (UO).

Figure \ref{fig:openx} illustrates the OpenX software architecture. Support for legacy applications, and specifically support for MPI applications is this thesis' target.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.75]{images/openx.png}
\caption[The OpenX Software Architecture]{The OpenX Software Architecture}
\label{fig:openx}
\end{figure}


\section{Design Principles}
\label{sec:design}

The main goal of this project that led to the development of HPX-RTE was to provide facilities for MPI applications to compile and run the exact same way in exascale software stack environment and be compatible with HPX runtime environment. This goal is achieved by replacing the current runtime environment (ORTE) with a new runtime environment developed from scratch to take advantage of the API provided by HPX. This choice was made by having a set of design principles in mind.

\begin{itemize}
\item \textbf{Modularity}\\
  Utilizing the modular structure of the Open MPI project, there is a framework dedicated to the runtime environment (rte) in OMPI layer. This framework is designed to provide the interfce necessary for different runtime environments (Figure \ref{fig:MCA-hpx-rte}). This allows different runtime environments to coexist independently inside the Open MPI project and be chosen by users based on their environment and application needs and priorities.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{images/MCA-hpx-rte.png}
\caption[Modular Component Architecure with rte Framework]{Modular Component Architecure with rte Framework}
\label{fig:MCA-hpx-rte}
\end{figure}
  
\item \textbf{Functionality}\\
  The main focus of this work is to have the MPI applications work on HPX infrastructure. To do this, most of the effort is put on having the functionality in place before tackling any possible consequences such as its effect on performance, efficiency, or power consumption. Although every possible consideration has been takeninto account not to introduce sources of performance degradation, such potential side effects could still happen. Those may be the topic for other studies after having the new runtime environment in place and functional.
  
\item \textbf{Simplicity}\\
  Simplicity is a key factor is the design of HPX-RTE. We have avoided introducing any unnecessary algorithm or functionality. Implementing the required API, we have tried to keep implementation as simple and straightforward as possible.

\item \textbf{Code Reuse}\\
  HPX-RTE relies on a number of advanced features and concepts provided by the HPX API. Therefore, there was no need to reimplement what was already done. We have tried to take those features to a new level by integrating them tightly into the implementation of HPX-RTE.

\end{itemize}


\section{Architecture}
\label{sec:architecture}

HPX-RTE is designed to replace the functionality of current Open MPI runtime (ORTE). Figure \ref{fig:open-mpi-layers-hpx-rte} illustates the logical layers of Open MPI software using \mbox{HPX-RTE} as its runtime environment. The modular component architecture of Open MPI facilitates the implementation of this design.

The required functinality for HPX-RTE is implemented as a component of the runtime environment (rte) framework (Figure \ref{fig:MCA-hpx-rte}) in OMPI layer. This design dictates a number of changes outside the rte framework. We will discuss those changes in the implementation section.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.45]{images/open-mpi-layers-hpx-rte.png}
\caption[Open MPI Layers using HPX-RTE]{Open MPI Layers using HPX-RTE}
\label{fig:open-mpi-layers-hpx-rte}
\end{figure}


\section{Runtime Environement Requirements}
\label{sec:rte-requirements}

The rte framework inside OMPI layer of Open MPI defines a required set of data structures and functions that every runtime environment component needs to provide implementation for~\cite{gabriel04:_open_mpi}:

\subsection{Process Name Objects and Operations}

\begin{enumerate}
\item \verb|ompi_jobid_t| \textbf{and} \verb|ompi_vpid_t|\\
  These two need to be defined as integer types. The jobid must be unique for a given \verb|MPI_COMM_WORLD| capable of connecting to another \verb|OMPI_COMM_WORLD| and the vpid will be the process's rank in \verb|MPI_COMM_WORLD|.

\item \verb|ompi_process_name_t|\\
  This is a struct that must contain at least two fields of type integer:
  \begin{enumerate}
  \item \verb|ompi_jobid_t| jobid
  \item \verb|ompi_vpid_t| vpid
  \end{enumerate}

\item \verb|OMPI_NAME_PRINT|\\
  When given a pointer to \verb|ompi_process_name_t|, this macro has to print a process name. The output format has to be a single string representing the name. This function should be thread-safe for multiple threads to call simultaneously.
  
\item \verb|OMPI_PROC_MY_NAME|\\
  A pointer to a global variable containing the \verb|ompi_process_name_t| for this process.
  
\item \verb|OMPI_NAME_WILDCARD|\\
  A wildcard name.
  
\item \verb|ompi_rte_compare_name_fields|\\
  A function used to compare fields in the \verb|ompi_process_name_t| struct. The function prototype must be of the form:
  \begin{lstlisting}[language=C]
  int ompi_rte_compare_name_fields(
        ompi_rte_cmp_bitmask_t mask,
        ompi_process_name_t *name1,
        ompi_process_name_t *name2);
  \end{lstlisting}
  The bitmask must be defined to indicate the fields to be used in the comparison. Fields not included in the mask must be ignored.
        Supported bitmask values must include:
        \begin{enumerate}
          \item \verb|OMPI_RTE_CMP_JOBID|
          \item \verb|OMPI_RTE_CMP_VPID|
          \item \verb|OMPI_RTE_CMP_ALL|
        \end{enumerate}

      \item \verb|uint64_t ompi_rte_hash_name|\textbf{(name)}\\
        Return a string hash uniquely representing the \verb|ompi_process_name| passed in.

      \item \verb|OMPI_NAME|\\
        An OPAL Data Packing Subsystem(DSS) constant for a handler already registered to serialize/deserialize an \verb|ompi_process_name_t| structure.

\end{enumerate}


\subsection{Collective Objects and Operations}

\begin{enumerate}
\item \verb|ompi_rte_collective_t|\\
  An OPAL object used during the runtime environment collective operations such as modex and barrier. This must be of type \verb|opal_list_item_t| and contain the following fields:
  
  \begin{enumerate}
  \item \verb|int32_t| id
  \item \verb|bool| active: A flag that user can poll on to know when collective operation has completed. If a user callback function is provided, this needs to be set to false just prior to calling it.
  \end{enumerate}

\item \verb|ompi_rte_modex|\\
  A function that performs an exchange of endpoint information to wireup the MPI transports. The function prototype must be of the form:
  \begin{lstlisting}[language=C]
  int ompi_rte_modex(ompi_rte_collective_t *coll);
  \end{lstlisting}
  At the completion of the modex operation, the \verb|coll->active| flag must be set to false, and the endpoint information must be stored in the modex database.
  This function must have barrier semantics across the \verb|MPI_COMM_WORLD| of the calling process.

\item \verb|ompi_rte_barrier|\\
  A function that performs a barrier operation within the RTE. The function prototype must be of the form:
  \begin{lstlisting}[language=C]
  int ompi_rte_barrier(ompi_rte_collective_t *coll);
  \end{lstlisting}
  At the completion of the barrier operation, the \verb|coll->active| flag must be set to false.
\end{enumerate}


\subsection{Process Information Structure}

\begin{enumerate}
\item \verb|ompi_process_info_t|
  A structure containing information about the current process. The following fields are mandatory within the struct:
  \begin{enumerate}
  \item \verb|app_num|
  \item \verb|pid|\\
    The current process's id. This should be same as the output of getpid() function.
  \item \verb|num_procs|\\
    Number of processes in this job (ie, \verb|MPI_COMM_WORLD|).
  \item \verb|my_node_rank|\\
    Relative rank on local node to other peers this runtime instance knows about. In a static job this will be \verb|my_local_rank|.
  \item \verb|my_local_rank|\\
    Relative rank of the process on the local node with other peers in this job (ie, \verb|MPI_COMM_WORLD|).
  \item \verb|num_local_peers|\\
    Number of local peers (peers in \verb|MPI_COMM_WORLD| on the same node).
  \item \verb|my_hnp_uri|
  \item \verb|peer_modex| \\
    This is a collective id for the modex operation.
  \item \verb|peer_init_barrier| \\
    A collective id for the barrier during \verb|MPI_Init|.
  \item \verb|peer_fini_barrier| \\
    A collective id for the barrier during \verb|MPI_Finalize|.
  \item \verb|job_session_dir|
  \item \verb|proc_session_dir|
  \item \verb|nodename| \\
    A string representation for the name of the node this process is located on.
  \item \verb|cpuset|
  \item \verb|ompi_process_info| \\
    A global instance of the \verb|ompi_process_t| structure.
  \end{enumerate}
  
\item \verb|ompi_rte_proc_is_bound|\\
  A global boolean that will be set to true if the runtime bound the process to a particular core or set of cores. Otherwise, it will be false.
\end{enumerate}


\subsection{Error Handling Objects and Operations}
\begin{enumerate}
\item
  \begin{lstlisting}[language=C]
  void ompi_rte_abort(int err_code, char *fmt, ...)
  \end{lstlisting}
  Abort the current process with the specified error code and message.

\item
  \begin{lstlisting}[language=C]
  int ompi_rte_abort_peers(ompi_process_name_t *procs,
                           size_t nprocs)
  \end{lstlisting}
  Abort the specified list of peers.
  
\item \verb|OMPI_ERROR_LOG(rc)|\\
  This is a macro that prints the error message regarding the given return code.
  
\item \verb|ompi_rte_register_errhandler|\\
  A function to register a callback function for the runtime environment to report asynchronous errors to the caller.
\end{enumerate}


\subsection{Initializing and Finalizing Objects and Operations}

\begin{enumerate}
\item 
  \begin{lstlisting}[language=C]
  int ompi_rte_init(int *argc, char ***argv);
  \end{lstlisting}
  This function initializes the runtime environment.
  
\item 
  \begin{lstlisting}[language=C]
  int ompi_rte_finalize(void);
  \end{lstlisting}
  This function finalizes the runtime environment.
  
\item
  \begin{lstlisting}[language=C]
  void ompi_rte_wait_for_debugger(void);
  \end{lstlisting}
  This function is called during \verb|MPI_Init|. It is used to wait for debuggers to do their pre-MPI attachment. This function will not block if no debugger is attached.
\end{enumerate}


\subsection{Database Operations}
\begin{enumerate}
\item
  \begin{lstlisting}[language=C]
  int ompi_rte_db_store(const ompi_process_name_t *proc,
                        const char *key,
                        const void *data,
                        opal_data_type_t type);
  \end{lstlisting}
  This function is used to store modex and other data in a local database. It is primarily used for storing modex data. The implementation of this function must store a copy of the data provided. The data is not guaranteed to be valid after return from the call.

\item
  \begin{lstlisting}[language=C]
  int ompi_rte_db_fetch(const struct ompi_proc_t *proc,
                        const char *key,
                        void **data,
                        opal_data_type_t type);
  \end{lstlisting}
  This function is used to fetch modex and other data from the database. Fetch accepts an \verb|ompi_proc_t|.

\item
  \begin{lstlisting}[language=C]
  int ompi_rte_db_fetch_pointer(
                  const struct ompi_proc_t *proc,
                  const char *key,
                  void **data,
                  opal_data_type_t type);  
  \end{lstlisting}
    
\item Pre-defined database keys (with associated values after \verb|rte_init|)
  \begin{enumerate}
    \item \verb|OMPI_DB_HOSTNAME|
    \item \verb|OMPI_DB_LOCALITY|
  \end{enumerate}
\end{enumerate}

%\subsection{Communication Support}

\section{Implementation}
\label{sec:implementation}
\iffalse
MPI system level adjustment, and change of configuration logic

what we have done?
Open MPI has an abstraction layer for runtime environments.
We provide that abstraction layer using hpx.
The abstraction is imperfect(lack of documentation,
not perfect interface independences)
Key components we had to develop?
- distributed key-value store and fetch using hpx actions
- barrier (synchronization point) using hpx actions
- translation table from hpx localities to mpi ranks
- populating some internal data structures
- extract and populate some key-value pairs into the db:
some are set in orte layer and used in ompi layer
- hacking the configure logic to detect and handle boost and HPX libraries
and modify the generated make files accordingly
- using c++ to compile a number of components
- manually exclude some components that are deeply entangled with rte
- intercepting printf from c code to see the output


current status:
- hpx 0.9.9
- openmpi 1.8.4
- restrctions: - one rank per locality
- launch with ssh
- communication protocols:
tcp: - tests (including communication) are running now
shared memory: -not tried recently, it did not work early on

questions/topics to discuss:
- os functionalities and interactio with hpx: fork, pipe, mmap
- slurm integration


\section{Challenges}
\label{sec:challenges}

%\subsection{C and C++}
%\subsection{Imperfect Abstraction in ORTE}
%\subsection{Fundamental Differences}
%\subsection{Configuration Logic}


\section{HPX-RTE Requirements}
open mpi
hpx
boost

features:
automatic detection of boost installation and hpx installation
\fi
