\chapter{Conclusion}
\label{sec:Conclusion}

High performance computing systems are growing toward hundreds of thousands to million node machines, utilizing the computing power of billions of cores. Running parallel applications on such large machines efficiently will require optimized runtime environments that are scalable and resilient. Multi and many core chip architectures in large scale supercomputers pose several new challenges to designers of operating systems and runtime environments.

HPX-RTE is a new, light weight, and open source runtime system specifically designed for the emerging exascale computing environment. The system is designed relying on HPX project advanced features such as asynchronous remote function calls (actions) and C++ futures to allow for easy extension and transparent scalability. HPX-RTE provides full compatibility for current MPI applications to run on HPX runtime system. 

\section{Performance}
Even though funcionality was the main priority in the design of HPX-RTE, our evaluations of HPX-RTE show better or equivalent performance compared to ORTE. We demonstrated the results of three different applications: Parallel computational fluid dynamics, hello world, and parallel smoothing.

\section{Future Extensions}
Current version of HPX-RTE is limited to one locality per node. Adding support for more than one locality (process) per node could be an extension in future versions.
Future developments of HPX-RTE can specifically target performance and make further improvements to the source code. 

HPX-RTE provides an easy and simple path for transition from MPI to HPX. 

\section{Hybrid Programming Models}
With the tight integration of HPX into Open MPI runtime provided by HPX-RTE, the possibility of hybrid programming models such as HPX-MPI is not far from reach. HPX-RTE also makes the transition from MPI to HPX easy. Application developers can replace parts of their code with HPX based implementation while their whole application functionality is maintained.

\section{More Evaluations}
better insight

optimization
and integration of more features from HPX into Open MPI.
