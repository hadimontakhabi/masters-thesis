\chapter{Introduction}
\label{sec:Introduction}

Traditionally, software is written for sequential computation. A problem is broken down into a series of instrcutions. Those instructions are executed sequentially on a single processor. In this scenario, only one instruction is executed at any given moment in time.~\cite{barney2010introduction}\\
Parallel computing is a type of computation in which calculations are carried out simultaneously to solve a computational problem.~\cite{Almasi:1989:HPC:160438}. The idea is to divide large problems into smaller ones and solve those smaller problems at the same time.


\section{Parallel Architectures}
Flynn's taxonomy is a classification of different computer architectures proposed by Michael J. Flynn in 1966\cite{5009071,44900}. This classification is based on the number of concurrent instructions and data streams available in the architecture.
There are four categories defined:

\begin{enumerate}
  \item \textbf{Single Instruction, Single Data Stream (SISD)}\\\
    In this architecture, a single processor executes a single instruction stream.
  \item \textbf{Single Instruction, Multiple Data Streams (SIMD)}\\\
    This architecture describes computers with multiple processing elements which execute the same operation on multiple data streams at the same time.
  \item \textbf{Multiple Instruction, Single Data Stream (MISD)}\\\
    In this architecture, multiple instructions operate on a single data stream. This architecture is almost non-existent, and it could be used for fault tolerant systems.
  \item \textbf{Multiple Instruction, Multiple Data Streams (MIMD)}\\\
    In this architecture, multiple independent processors execute different instructions on different data streams at the same time. This is the most generic category.
    
    MIMD category is subdivided into shared-memory and distributed memory architectures. Furthermore, shared-memory architectures can be classified as symmetric multiprocessors with uniform memory access (UMA) or as non-uniform memory access (NUMA) architectures that have recently gained popularity~\cite{kshemkalyani2008distributed,sutter2005free}.

\end{enumerate}


\section{Parallel Programming Models}
The programming model is an abstraction that defines the hardware and the form of programming language or API used for writing parallel programs.
Parallel programming models can be divided into two broad areas: Process interaction and problem decomposition.~\cite{parallelprogrammingmodels}
\begin{itemize}
\item \textbf{Process Interaction}\\
  Parallel processes need to communicate with each other. The mechanism that they use for the communication is described as process interation. Two main types of process interaction are shared memory and message passing. The interaction could also be implicit.
  \begin{itemize}
  \item \textbf{Shared Memory}\\
    In this model, parallel processes share a global address space which they all read and write to. Since these operations are asynchronous, protection mechanisms like locks and semaphores are needed to control concurrent access.
  \item \textbf{Message Passing}\\
    In this model, parallel tasks exchange data through sending and receiving messages to and from one another. These communications can be synchronpus or asynchronus.
  \item \textbf{Implicit}\\
    In this model, process interactions are not visible to the programmer. Usually, the compiler or the runtime is responsible for performing the interations.
  \end{itemize}
\item \textbf{Problem Decomposition}\\
Problem decomposition describes the way processes are expressed to break down a large problem~\cite{Quinn:2003:PPC:1211440}.
  \begin{itemize}
  \item \textbf{Task Parallelism}\\
    The main focus of a task based parallelism is on processes or threads as individual units of execution. This is a natural way to express message passing communication.
  \item \textbf{Data Parallelism}\\
    In task-parallel models the data is usually structured in an array. A set of tasks will operate independently on separate partitions of the data.     
  \end{itemize}
\end{itemize}


\section{Message Passing Interface (MPI)}
Message Passing Interface (MPI) is a language-independent communication protocol used for programming in parallel environments. ``MPI is a specification for a standard library for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists.''~\cite{gropp1996high} MPI has become a de facto standard for communication aming processes running on a distributed memory system.

``The goal of the Message Passing Interface is to establish a portable, efficient, and flexible standard for message passing that will be widely used for writing message passing programs. As such, MPI is the first standardized, vendor independent, message passing library. The advantages of developing message passing software using MPI closely match the design goals of portability, efficiency, and flexibility. MPI is not an IEEE or ISO standard, but has in fact, become the ``industry standard'' for writing message passing programs on HPC platforms.''~\cite{barker2015message} There are several implementations of MPI, including some that are in the public domain and some proprietary implementations from companies like HP, Intel, and Microsoft.


\section{Runtime Environments and MPI Jobs}
High performance computing systems are growing toward hundreds of thousands to million node machines, utilizing the computing power of billions of cores. Running parallel applications on such large machines efficiently will require optimized runtime environments that are scalable and resilient. Considering a future where MPI remains a major programming paradigm, the MPI implementations will have to seamlessly adapt to launching and managing large scale applications on resources considerably larger than today.~\cite{bosilca2011scalability}

\section{Challenges for Runtime Environments}
Multi and many core chip architectures in large scale supercomputers pose several new challenges to designers of operating systems and runtime environments.\\
``Operating systems and runtime environments on supercomputers have similar goals: both seek to provide an environment for executing applications in a scalable and high-performing way. Achieving this goal often requires minimizing the layers of indirection between the application and the architecture.''~\cite{Hoefler:2012:OSR:2237840.2237848}\\
Developing a software environment to support high performance computing applications in today's distributed systems poses a significant challenge. The runtime environment (RTE) must be capable of supporting heterogeneous operations, scale from one to large numbers of processors in an efficient manner, and provide strategies for dealing with fault scenarios that are expected of computing systems effectively. Furthermore, the runtime must be easy to use, providing users with a transparent interface to the computing environment in a manner that avoids the need to customize applications when moving between specific computing resources.~\cite{Castain2008153}

\section{Organization of the Document}
Chapter 2 provides a background and the state of art approaches to parallel runtime enviroment challenges. Open MPI as an implementation of choice will be introduced. We will discuss the project features and its architecture. Open Runtime Environment (ORTE), the current runtime environment for Open MPI project, and its features will be discussed in detail in secion 2.2. ParalleX and High Perfomance ParalleX (HPX) will be presented in section 2.3. Chapter 3 covers the design philosophy, architecture, and implementation of our runtime environment for Open MPI (HPX-RTE) in sections 3.1, 3.2, and 3.3 accordingly.

