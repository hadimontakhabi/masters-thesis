\chapter{Background}
\label{sec:Background}

\section{Open MPI}
\label{sec:openmpi}
``The Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners. Open MPI is therefore able to combine the expertise, technologies, and resources from all across the High Performance Computing community in order to build the best MPI library available. Open MPI offers advantages for system and software vendors, application developers and computer science researchers.\\
Features implemented or in short-term development for Open MPI include:
\begin{itemize}
  \item Full MPI-3 standards conformance
  \item Thread safety and concurrency
  \item Dynamic process spawning
  \item Network and process fault tolerance
  \item Support network heterogeneity
  \item Single library supports all networks
  \item Run-time instrumentation
  \item Many job schedulers supported
  \item Many OS's supported (32 and 64 bit)
  \item Production quality software
  \item High performance on all platforms
  \item Portable and maintainable
  \item Tunable by installers and end-users
  \item Component-based design, documented APIs
  \item Active, responsive mailing list
  \item Open source license based on the BSD license''~\cite{openmpi}
\end{itemize}
    
\subsection{The Architectire of Open MPI}
Open MPI is built based on a component architecture called the Modular Component Architecture (MCA). Component based archtirecture makes large software projects extensible and maintainable~\cite{barrett2005analysis,graham2006open}. It also allows users to build their own costumized components and integrate it into Open MPI. Component based architectures are popular in the high performance computing community~\cite{squyres2003component,bernholdt2006component}.

Open MPI is comprised of three main functional areas (Figure \ref{fig:MCA_framework})~\cite{graham2006open}:

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{images/MCA_framework.png}
\caption{MCA, component frameworks, and the components}
\label{fig:MCA_framework}
\end{figure}

\begin{enumerate}
\item \textbf {MCA}\\
  The backbone modular component architecture that provides management services for all other layers.
  
  The MCA is responsible for management of the component frameworks and providing them services they use. For instance, the MCA provides the ability to accept run-time parameters from higher-level abstractions (e.g., mpirun) and pass them down through the component framework to individual components. It also finds components at build-time and invokes their corresponding hooks for configuration, building, and installation.
  
\item \textbf{Component frameworks}\\
  Each major functional area in Open MPI has a corresponding back-end component framework, which manages modules.
  
  Each component framework is a construct that is created for a single, targeted task. For example, \textbf{btl} (Byte Transfer Layer) framework is used to send and receive data on different types of networks, \textbf{allocator} framework is responsible for memory allocation, and \textbf{coll} framework which is dedicated to MPI collective algorithms. A framework uses MCA's services to discover, load, use, and unload components at run time. Each framework has different policies and use case scenarios; some only use one component at a time while others use all available components simultaneously.

\item \textbf {Components}\\
  Self-contained software units that can configure, build, and install themselves. A component is an implementation of a framework's interface. Components are also known as ``plugins''. Each instance of a component is called a ``module''. 

  The Open MPI software has three classes of components: Open MPI (OMPI) components, Open Runtime Environment (ORTE) components, and Open Portable Access Layer (OPAL) components. (Figure \ref{fig:open-mpi-layers})

  Frameworks, components, and modules can be either dynamic or static. This means, they can be available as plugins or they can be compiled statically into libraries.
\end{enumerate}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{images/open-mpi-layers.png}
\caption{Open MPI layers}
\label{fig:open-mpi-layers}
\end{figure}


\section{Open Runtime Environment (ORTE)}
\label{sec:orte}
Developing software environments for high performance computing applications in heterogenous distributed systems poses a significant challenge. The runtime environment (RTE) must be capable of supporting heterogeneous operations, efficiently scaling from one to large numbers of processors, and providing effective strategies for dealing with fault scenarios that are expected as our systems continue to scale~\cite{kronstadt2005peta}.

There has been a number of studies with different approaches to this challenge. Each approach focuses on a particular aspect of the overall problem. For example, LAM/MPI emphasis was on ease of portability and performance~\cite{squyres2004component}. LA-MPI focused on data fault tolerance~\cite{aulwes2004architecture}, and HARNESS FT-MPI focused on system fault tolerance~\cite{fagg2002harness}.

``The major roles of the runtime are:
\begin{enumerate}
\item \textbf{Launch}\\
  Launch the MPI applicationâ€™s processes. This role is shared between the implementation runtime and the parallel machine scheduling/launching mechanism.
\item \textbf{Connect}\\
  Help the MPI library establish the necessary connections between the processes. Depending on the network used, hardware or configuration specific issues, the connection information (also known as the business card, or the URI of
a process) may not be known before the MPI processes are launched. It is then necessary to distribute this information through an out-of-band messaging system. In most MPI implementations, this role is dedicated to the runtime.
\item \textbf{Control}\\
  Control the MPI processes: ensure that in case of a crash the entire environment is gracefully cleaned; depending on the operating system and implementation, forward signals to the MPI processes; ensure that completion codes
are returned to the user command: mpirun.
\item \textbf{IO}\\
  Forward the standard input/output: users usually expect that the information printed on the standard output should appear on the standard output of the command they launched. Because the command they launched does not necessarily run on the same machine as where the print is issued in an MPI application, it is necessary to ensure the transmission of this information.''~\cite{bosilca2011scalability}
\end{enumerate}

The Open Runtime Environment (ORTE) was developed as a part of the Open MPI project to support distributed high performance computing applications operating in a heterogeneous environment. Implementation of the ORTE is based on the Modular Component Architecture (MCA). The main design objectives of the ORTE are ease of use, resilient operations, scalability, and extensibility. Interprocess communication, resource discovery and allocation, and process launch across different platforms in a transparet manner are main features of the ORTE.\cite{castain2005open}.

The ORTE consists of four major subsystems (Figure \ref{fig:orte-architecture})\cite{castain2005open,castain2008open,Castain2008153}:

\begin{enumerate}
\item \textbf{General Purpose Registry (GPR)}\\
  The GPR is the core subsystem in the ORTE architecture. It provides a mechanism for exchanging of communication connection data, in the form of key-value pairs, among processes.  The GPR is also used to synchronize events across the system. It asynchronously notifies subscribers of events such as data changes in the registry or new data being entered to the registry.
\item \textbf{Resource Management}\\
  The Resource management subsystem consists four smaller subsystems: \textit{Resource Discovery Subsytem (RDS)}, \textit{Resource Allocation Subsystem (RAS)}, \textit{Resource Mapping Subsystem}, and \textit{Process Launch Subsystem (PLS)}. These four subsystems together provide services for resource discovery, allocation, mapping, and process launch. 
\item \textbf{Error Management}\\
  The \textit{State Monitoring and Reporting (SMR)} subsystem and the \textit{Error Manager} subsystem are two smaller subsystems constructing the error management subsystem. The error management subsystem brings the fault tolerance capability to the ORTE.
\item \textbf{Support Services}\\
  There are four subsystems comprising the support services subsystem: The \textit{Runtime Messaging Layer (RML)} is responsible for providing administrative communication services across all ORTE subsystems. The \textit{Name Services (NS)} subsystem assigns each application, and each process within each application, a unique identifier. The \textit{I/O Forwarding (IOF)} subsystem is responsible for transporting standard input, output, and error messages between the remote processes and the user. And finally, the \textit{Data Services (DS)} subsystem is responsible for facilities like providing a single interface for all declared data types, packing/unpacking network communications, and support for transparent data manipulation within the ORTE.
\end{enumerate}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{images/orte.png}
\caption{The ORTE architecture~\cite{hpcwire}}
\label{fig:orte-architecture}
\end{figure}

\section{ParalleX}
\label{sec:parallex}
ParalleX~\cite{kaiser2009parallex} is a new computation model that attempts to address the underlying sources of performance degradation~\cite{sterling2010enabling}:

\begin{enumerate}
\item \textbf{Starvation}\\
  Starvation is the phenomenon of resourses being idle performing no useful action because the amount of concurrent work available to the resources is insufficient to utilize all of them.
\item \textbf{Latency}\\
  Latency is the amount of time a message takes to traverse a system. Accessing remote resources imposes a minimum delay equivalent to latency.
\item \textbf{Overhead}\\
  Management of parallel resources requires extra work which is not necessary in the case of utilizing sequential resources.
\item \textbf{Waiting}\\
  In systems with shared resources, the possibility of contention exists. When there is such a contention, it needs to be resolved. Hence, there is a delay due to waiting for contention resolution.
\end{enumerate}

ParalleX also tries to address the difficulties of programmer productivity like explicit locality management and scheduling, performance tuning, fragmented memory, and synchronous global barriers to dramatically enhance the broad effectiveness of parallel processing for high end computing~\cite{4228212}. ParalleX changes the fundamental model of parallel computation from communicating sequential processes (e.g., MPI) to an innovative combination of concepts using message-driven work-queue execution in the context of a global address space.~\cite{tabbal2011preliminary,sterling2014towards}

Main components of ParalleX include~\cite{gao2007parallex,kaiser2009parallex,dekate2012improving}:
\begin{enumerate}
\item \textbf{Active Global Address Space (AGAS)}\\
  While avoiding the overhead of cache coherence, the AGAS extends the PGAS \cite{stitt2009introduction} models (GASNet~\cite{bonachea2002gasnet}, UPC~\cite{upc2005upc}) by allowing the dynamic migration of first class objects across the physical system without having to change the object's name. This facilitates load balancing by allowing work to be migrated from heavily loaded nodes to less loaded nodes.  
\item \textbf{Parallel Processes}\\
  Unlike conventional models, ParalleX processes span over multiple nodes and share nodes as well. A ParalleX process can define a name space shared across several localities supporting many concurrent threads and child processes.
\item \textbf{Threads}\\
  ParalleX threads provide local control flow and data usage within a single node utilized for specifying and performing most of the computational work to be performed by an application program. ParalleX threads can migrate to remote localities. 
\item \textbf{Local Control Objects (LCOs)}\\
  LCOs provide different functionalities for event driven ParalleX thread creation, protection of data structures from race conditions, and scheduling of work automatically to let every single computation strand proceed as far as possible.
\item \textbf{Parcels}\\
  Parcels are messages that carry action and data asynchronously between different localities. ``Parcels enable message passing for distributed control flow and dynamic resource management, implementing a split phase transaction based execution model.''~\cite{kaiser2009parallex}
\item \textbf{Percolation}\\
  Percolation is a technique for using resources by moving the work to the resource while both hiding the latency of such action and eliminating the overhead of such action from the target resource. 
\end{enumerate}


\section{High Performance ParalleX (HPX)}
\label{sec:hpx}
High Performance ParalleX (HPX) is the first open source general purpose C++ runtime system implementation for the ParalleX execution model~\cite{huck2013early,kaiser2014hpx}.

HPX, like many recent programming models is based on lightweight tasks. Task based parallel programming models can be divided into three major categories~\cite{kaiser2014hpx,podobas2010comparison}:

\begin{itemize}
  \item \textbf{Libraries}\\
    Intel TBB~\cite{pheatt2008intel}, Qthreads~\cite{wheeler2008qthreads}, and StarPU~\cite{augonnet2011starpu} are some known examples for library solutions.
  \item \textbf{Language Extensions}\\
    OpenMP~\cite{dagum1998openmp} and Intel Cilk Plus~\cite{robison2012cilk} are examples of language extensions.
  \item \textbf{Experimental Programming Languages}\\
    Chapel~\cite{chamberlain2007parallel}, X10~\cite{pharr2012ispc}, and Intel ISPC~\cite{pharr2012ispc} are notable examples in this category.
\end{itemize}

The majority of the mentioned task based programming models focus on node level parallelism. Providing a solution for homogeneous execution of local and remote operations is what distinguishes HPX from those models.

``HPX represents an innovative mixture of a global system-wide address space, fine grain parallelism, and lightweight synchronization combined with implicit, work queue based, message driven computation, full semantic equivalence of local and remote execution, and explicit support for hardware accelerators
through percolation.''~\cite{kaiser2014hpx}

\subsection{HPX Design Principles}
HPX follows a set of design principles that have been around for years. However, HPX gathers all these principles into a unified system~\cite{kaiser2014hpx}.

\begin{enumerate}
\item \textbf{Latency Hiding instead of Latency Avoidance}\\
  It is impossible to have no latency in a system. However, to hide the latency, some unrelated useful work can be done during that time. This is one of the main concepts integrated into the design of HPX.
  
\item \textbf{Fine-grained Parallelism instead of Heavyweight Threads}\\
  To hide latencies for very short operations, low overhead of context swithing is a must. The overall system utilization improves by smaller overhead of a context switch and finer granularity of the threading system.
  
\item \textbf{Constrained Based Synchronization to Replace Global Barriers}\\
  Having all threads or processes wait for a particular operation to be finished is waste of resources that could otherwise be utilized. Replacing such barriers with constraint based synchronization through dataflow techniques could improve the overall performance of a system.
  
\item \textbf{Adaptive Locality Control instead of Static Data Distribution}\\
  MPI leaves the responsibility of data distribution to the programmer. The easy approach for most programmers is to distribute the data statically. PGAS systems rely on static data distribution as well. When there is an imbalance in the workload, this becomes a problem. In such cases, migrating part of the application data to different localities (nodes) by the runtime system could increase overall utilization and make programmers job easier.
  
\item \textbf{Moving Work to the Data instead of Moving Data to the Work}\\
  It is obvious that the amount of data for a particular operation is very often much smaller than the amount of data the operation is performed on. Although this is possible using MPI, it is not deeply built into MPI model. Having a system that could provide this capability would reduce overhead of unnecessary data movement.
  
\item \textbf{Message Driven Computation instead of Message Passing}\\
  In message passing models like MPI, the receiver of a message needs to spend time waiting for the incoming messages. However, message driven computation allows sending messages without the receiver actively waiting for them. Incoming message are handled asynchronously. 
  
\end{enumerate}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{images/hpx.png}
\caption{Modular Structure of HPX Implementation~\cite{kaiser2009parallex}}
\label{fig:hpx-structure}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.7]{images/hpx_the_api.png}
\caption{Overview of the main API exposed by HPX~\cite{kaiser2014hpx}}
\label{fig:hpx-api}
\end{figure}
